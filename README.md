# LLM Learning Project

A comprehensive exploration of Large Language Model training and reinforcement learning techniques, with focus on GRPO (Group Relative Policy Optimization) for mathematical reasoning tasks.

## ğŸ“ Project Structure

```
learning_llm/
â”œâ”€â”€ grpo/                           # Original GRPO implementation
â”‚   â”œâ”€â”€ trainer.py                  # Legacy GRPO trainer
â”‚   â””â”€â”€ reward.py                   # Reward function utilities
â”‚
â”œâ”€â”€ training_with_unsloth/          # Production GRPO training framework
â”‚   â”œâ”€â”€ framework/                  # Core training infrastructure
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py        # Refactored GRPO trainer with detailed eval
â”‚   â”‚   â”œâ”€â”€ config.py              # Experiment configuration management
â”‚   â”‚   â””â”€â”€ dataset.py             # GSM8K dataset utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ rewards/                    # Reward function library
â”‚   â”‚   â””â”€â”€ reward_functions.py    # Correctness, format, and mark rewards
â”‚   â”‚
â”‚   â”œâ”€â”€ experiments/                # Training scripts
â”‚   â”‚   â”œâ”€â”€ run_experiment.py      # Main experiment runner
â”‚   â”‚   â””â”€â”€ run_eval_test.py       # Evaluation-focused training
â”‚   â”‚
â”‚   â””â”€â”€ experiment_configs/         # YAML configurations
â”‚       â”œâ”€â”€ eval_test_improved.yaml # Recommended config (30 train/20 test)
â”‚       â””â”€â”€ qwen3_grpo_*.yaml      # Production configs
â”‚
â””â”€â”€ .cursor/                        # Documentation
    â””â”€â”€ rules                       # WandB logging standards
```

## ğŸ¯ Key Features

### 1. **GRPO Training Pipeline**
- Group Relative Policy Optimization for LLM fine-tuning
- Multiple reward functions (correctness, format, digit validation)
- KL divergence regularization with reference model
- Gradient accumulation and checkpointing for memory efficiency

### 2. **Efficient 4B Model Training on 16GB**
- **QLoRA (4-bit quantization)**: 2GB base model (vs 8GB FP16)
- **LoRA adapters**: Train 100M params instead of 4B
- **Gradient checkpointing**: 5x memory reduction on activations
- **Unsloth optimizations**: 2x faster training with custom kernels

### 3. **Comprehensive Evaluation**
- **Pass@K metrics**: Pass@1 (accuracy), Pass@5, Pass@10
- **Detailed logging**: Every answer for every eval question
- **Baseline comparison**: Pre-training vs post-training analysis
- **WandB integration**: Real-time metrics and visualization

### 4. **GSM8K Mathematical Reasoning**
- Grade school math word problems
- Configurable train/test splits
- Robust answer extraction and normalization
- Format enforcement with partial credit

## ğŸš€ Quick Start

### Installation
```bash
# Install dependencies
pip install torch transformers datasets wandb unsloth bitsandbytes

# Or use uv (recommended)
uv sync
```

### Run Training
```bash
cd /path/to/learning_llm
UV_PYTHON=python3 uv run python3 \
  training_with_unsloth/experiments/run_eval_test.py \
  training_with_unsloth/experiment_configs/eval_test_improved.yaml
```

### Configuration
Edit `experiment_configs/eval_test_improved.yaml`:
```yaml
model_name: Qwen/Qwen3-4B-Instruct-2507
train_samples: 30
test_samples: 20
learning_rate: 5e-5
num_epochs: 100
temperature: 0.8
eval_temperature: 0.5
beta: 0.05  # KL regularization
```

## ğŸ“Š Key Metrics

### Pass@K Definition
- **Pass@1**: Greedy decoding accuracy (temperature=0)
- **Pass@5**: Success if â‰¥1 of 5 samples correct (temperature=0.5)
- **Pass@10**: Success if â‰¥1 of 10 samples correct (temperature=0.5)

### Expected Performance
```
Baseline (pre-trained Qwen3-4B):
  Pass@1:  0.75-0.85
  Pass@5:  0.85-0.90
  Pass@10: 0.90-0.95

After GRPO training (30 samples, 10 epochs):
  Pass@1:  0.85-0.92
  Pass@5:  0.90-0.95
  Pass@10: 0.92-0.97
```

## ğŸ› Critical Bug Fixes

### normalize_number() Fix
**Issue**: Model answers marked wrong due to formatting
```python
# Before: '$108' â‰  '108' â†’ Marked WRONG
# After:  '$108' â†’ '108' â†’ Marked CORRECT âœ“
```

**Impact**: +40% improvement on Pass@1 baseline

See `CRITICAL_BUG_FIX.md` for details.

## ğŸ’¡ Technical Highlights

### Memory Optimization
```
Full Fine-Tuning:  50GB VRAM âŒ
LoRA:             15GB VRAM âš ï¸
QLoRA (4-bit):     8GB VRAM âœ“ (fits in 16GB!)
```

### Reward Functions
1. **correctness_reward** (2.0): Numerical answer matches ground truth
2. **digit_reward** (0.5): Answer contains only digits
3. **hard_format_reward** (0.25-0.5): Uses `<answer>` tags (partial credit)
4. **mark_reward** (0.5): Complete `<think>...</think><answer>...</answer>` format

### GRPO Loss Components
```python
# Advantage-based policy gradient with clipping
ratio = exp(log_prob_new - log_prob_old)
clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)
loss = -min(ratio Ã— advantage, clipped_ratio Ã— advantage)
```

## ğŸ“ˆ WandB Logging

All experiments log to WandB with:
- Training: `grpo_loss`, `rewards/{mean,std,max,min}`, `timestamp`
- Evaluation: `eval/pass@{1,5,10}`, `epoch/accumulated_reward`
- Samples: `sample/{prompt,response,answer}`

View at: https://wandb.ai/[your-entity]/qwen3_4b_grpo

## ğŸ“š Documentation

- `GRPO_TRAINING_EXPLANATION.md`: Deep dive into GRPO algorithm
- `CRITICAL_BUG_FIX.md`: normalize_number() bug analysis
- `DETAILED_EVAL_LOGGING.md`: Evaluation system guide
- `IMPROVEMENTS_SUMMARY.md`: All optimizations applied
- `.cursor/rules`: WandB logging standards

## ğŸ”¬ Experiments

### Recommended Starting Config
- **Model**: Qwen/Qwen3-4B-Instruct-2507
- **Dataset**: GSM8K (30 train, 20 test)
- **Learning Rate**: 5e-5
- **Batch Size**: 1 (effective 4 with gradient accumulation)
- **KL Beta**: 0.05 (prevents model drift)
- **Temperature**: 0.8 (training), 0.5 (eval)

### Hyperparameter Tuning Tips
- â†‘ LR if Pass@1 not improving (try 1e-4)
- â†‘ Temperature if Pass@5 = Pass@1 (try 0.9)
- â†‘ Beta if training unstable (try 0.1)
- â†‘ Samples if dataset too small (try 100+)

## ğŸ› ï¸ Development

### Running Tests
```bash
# Test reward functions
python3 -c "from training_with_unsloth.rewards.reward_functions import *; ..."

# Test normalization
python3 -c "from training_with_unsloth.rewards.reward_functions import normalize_number; print(normalize_number('\$108'))"
```

### Adding New Reward Functions
```python
# In rewards/reward_functions.py
def my_reward(prompts, responses, answers):
    """Custom reward function."""
    rewards = []
    for response in responses:
        # Your logic here
        reward = compute_reward(response)
        rewards.append(reward)
    return rewards

# In run_experiment.py
reward_funcs = [correctness_reward, my_reward, ...]
```

## ğŸš¨ Common Issues

### OOM (Out of Memory)
```yaml
# Reduce memory usage:
batch_size: 1
gradient_accumulation_steps: 8  # Increase this
num_generations: 2  # Reduce this
load_in_4bit: true  # Must be true
use_gradient_checkpointing: true  # Must be true
```

### Pass@1 Not Improving
1. Check answer normalization (dollar signs, units?)
2. Increase learning rate (5e-5 â†’ 1e-4)
3. More training samples (30 â†’ 100+)
4. Check reward signals are non-zero

### Pass@5 = Pass@1
- Increase `eval_temperature` (0.5 â†’ 0.8)
- Check if samples are actually different
- May indicate model is very confident

## ğŸ“– References

- **GRPO Paper**: [Group Relative Policy Optimization]
- **QLoRA**: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- **GSM8K**: [Grade School Math 8K Problems](https://github.com/openai/grade-school-math)
- **Unsloth**: [2x Faster LLM Training](https://github.com/unslothai/unsloth)

## ğŸ¤ Contributing

This is a learning project. Key areas for improvement:
- [ ] Add more reward functions (step-by-step reasoning)
- [ ] Experiment with other models (Llama3, Mistral)
- [ ] Try other datasets (MATH, AQuA)
- [ ] Implement PPO comparison
- [ ] Add model merging after training

## ğŸ“ License

Educational project for learning purposes.

## ğŸ™ Acknowledgments

- OpenAI for GSM8K dataset
- Unsloth team for optimization framework
- Hugging Face for transformers library
- WandB for experiment tracking


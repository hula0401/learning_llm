# GRPO Training Behavior Explanation

## Why Only 3 Samples Shown?

**Answer:** This is intentional for logging brevity.

- The code generates `num_generations=4` responses per prompt
- But only prints `min(3, len(responses))` samples to avoid cluttering logs
- All 4 responses are still processed and contribute to training

**Code location:** `rewards/reward_functions.py:34`
```python
k = min(3, len(responses))  # Only print first 3 for readability
```

---

## Why Is Correctness Reward 0 When Model Outputs "200"?

### Current Behavior (Early Training)

**Model Output:**
```
' answer </answer>\n\n'
```

The model is generating the literal word "answer" instead of the numerical answer!

### Why This Happens

1. **Model hasn't learned yet**: Early in training, the model doesn't know the expected format
2. **Stop string works**: Generation correctly stops at `</answer>` tag
3. **But content is wrong**: Model outputs "answer" instead of computing "200"

### Reward Breakdown

| Reward Function | Score | Reason |
|----------------|-------|--------|
| correctness_reward | 0.0 | "answer" â‰  "24" (correct) |
| digit_reward | 0.0 | "answer" is not a digit (correct) |
| hard_format_reward | 0.0 | Missing `<think>...</think><answer>...</answer>` (correct) |
| mark_reward | 0.125 | Partial credit for having `</answer>\n` tag âœ“ |

### Expected Learning Progression

As training continues, the model will learn to:

1. **Generate thinking**: `<think>\n24 + 6 = 30\n</think>\n`
2. **Generate answer**: `<answer>\n30\n</answer>\n`
3. **Maximize rewards**: Get 2.0 + 0.5 + 0.5 + 0.5 = 3.5 total reward!

---

## GRPO Gradient Flow Verification

### Question: Do rewards actually update model parameters?

**Answer: YES!** Here's the exact gradient flow:

```
Rewards â†’ Advantages â†’ Loss â†’ Gradients â†’ Model Parameters
```

### Step-by-Step Breakdown

1. **Reward Computation** (`generate_experiences`):
   ```python
   rewards = [0.125, 0.125, 0.5, 0.125]  # For 4 generations
   advantages = (rewards - mean) / std
   # Example: [âˆ’0.7, âˆ’0.7, +1.4, âˆ’0.7]
   ```

2. **Loss Computation** (`compute_loss`):
   ```python
   # Policy ratio: How much policy changed
   ratio = exp(new_log_prob - old_log_prob)
   
   # Weighted by advantage (contains reward signal!)
   loss = -min(ratio * advantage, clipped_ratio * advantage)
   ```

3. **Backpropagation** (`train_step`):
   ```python
   loss.backward()  # Computes gradients w.r.t. model parameters
   optimizer.step()  # Updates model weights
   ```

### Key Insight

- **High reward** â†’ **Positive advantage** â†’ Model learns to **increase** probability of that response
- **Low reward** â†’ **Negative advantage** â†’ Model learns to **decrease** probability of that response

This is how GRPO guides the model toward higher-reward responses!

---

## Comma Formatting in Numbers

### Question: Does "360,000" == "360000"?

**Original Answer: NO**

The string `"360,000"` would not match `"360000"` in exact string comparison.

### Solution

Added `normalize_number()` function:

```python
def normalize_number(text: str) -> str:
    """Remove commas from numbers."""
    # "360,000" â†’ "360000"
    normalized = re.sub(r'(\d),(\d)', r'\1\2', text)
    return normalized.strip()
```

Now both formats match:
- Model outputs: `"360,000"` â†’ normalized â†’ `"360000"` âœ“
- Ground truth: `"360000"` â†’ normalized â†’ `"360000"` âœ“
- **Match! Reward = 2.0**

---

## Dataset Answer Extraction

### GSM8K Answer Format

```
"Step 1: 100,000 * 0.01 = 1,000\n
Step 2: 150 * 10 = 1,500\n
Step 3: 1,500 - 1,000 = 500\n
#### 200"
```

### Extraction Logic

The dataset now extracts only the final answer after `####`:

```python
def extract_final_answer(answer_text: str) -> str:
    match = re.search(r"####\s*([^\n]+)", answer_text)
    if match:
        return match.group(1).strip()  # Returns "200"
    return answer_text.strip()
```

This ensures the ground truth is just `"200"`, not the entire reasoning chain.

---

## WandB Logging

### Metrics Logged Per Step

```python
wandb.log({
    # Reward statistics
    "rewards/mean": 0.125,      # Average reward in this group
    "rewards/std": 0.0,          # Std deviation of rewards
    "rewards/max": 0.125,        # Best reward
    "rewards/min": 0.125,        # Worst reward
    
    # Training loss
    "grpo_loss": 0.0,            # GRPO policy loss
    "step": 0,                   # Training step number
})
```

These metrics help you monitor:
- Are rewards increasing over time?
- Is the model learning to generate better responses?
- Is the loss converging?

---

## Summary

âœ… **Training is working correctly!**

The model:
1. Generates short responses (stop string working)
2. Gets low rewards (expected early in training)
3. Computes gradients from rewards (verified gradient flow)
4. Updates parameters to maximize future rewards

Over time, the model will learn to:
- Generate proper `<think>` reasoning
- Compute correct numerical answers
- Format responses correctly
- Maximize total reward (up to 3.5 per response)

Be patient - GRPO training takes time to converge! ðŸš€

